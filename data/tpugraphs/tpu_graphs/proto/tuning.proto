/* Copyright 2023 The tpu_graphs Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

syntax = "proto3";

package tpu_graphs;

import "third_party/tensorflow/compiler/xla/service/hlo.proto";
import "third_party/tensorflow/compiler/xla/xla_data.proto";

// All tuning data for multiple HLO modules.
message TuningData {
  repeated ModuleTuningData modules = 1;
}

// Tuning data for an HLO module.
message ModuleTuningData {
  xla.HloModuleProto module = 1;
  uint64 fingerprint = 2;
  repeated ConfigProfile runs = 3;
  // For graph-level configuration, config_index_to_node(i) indicates
  // the node id that the config at index i belongs to.
  repeated uint64 config_index_to_node = 4;
}

// Configuration and its information including runtime profile.
message ConfigProfile {
  oneof config {
    HloOpConfig op_config = 1;
    HloModuleConfig module_config = 2;
  }
  bool error = 3;
  xla.ExecutionProfile profile = 4;
  bool is_default = 5;
}

// The HloOpConfig is used to configure different parameters that affect the
// performance of the HLO.
// Every op in an HLO module that can be tuned will have its own corresponding
// HloOpConfig.
message HloOpConfig {
  // The tile size applicable for this op.
  TileSizeConfig tile_size_config = 1;
}

message TileSizeConfig {
  repeated int64 kernel_bounds = 1;
  repeated int64 output_bounds = 2;
  repeated int64 input_bounds = 3;
  repeated int64 iteration_bounds = 4;
}

// The HloModuleConfig is used to configure different global decisions that
// affect the performance of the entire HLO module.
message HloModuleConfig {
  optional LayoutConfig layout_config = 1;
}

message LayoutConfig {
  message NodeLayoutConfig {
    message TensorLayoutConfig {
      repeated int32 dims = 1;
    }
    repeated TensorLayoutConfig tensors = 1;
  }
  // Configuration for each relevant operation, sorted topologically and by
  // computation content.
  repeated NodeLayoutConfig nodes = 1;
}
